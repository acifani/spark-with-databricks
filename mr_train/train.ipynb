{"cells":[{"cell_type":"code","source":["import urllib\n\ntracks_resp = urllib.urlopen('https://www.mapr.com/ebooks/spark/data/tracks.csv')\ncustomers_resp = urllib.urlopen('https://www.mapr.com/ebooks/spark/data/cust.csv')"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["dbutils.fs.mkdirs(\"/tmp/tracks\")\ndbutils.fs.put(\"/tmp/tracks/tracks.csv\", tracks_resp.read(), overwrite=True)\ndbutils.fs.put(\"/tmp/tracks/customers.csv\", customers_resp.read(), overwrite=True)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["## Files format\n\n#### Tracks dataset\n| Field Name | Event ID | Customer ID | Track ID | Datetime            | Mobile  | Listening Zip |\n|------------|----------|-------------|----------|---------------------|---------|---------------|\n| **Type**   | Integer  | Integer     | Integer  | String              | Integer | Integer       |\n| **Example**| 9999767  | 2597        | 788      | 2014-12-01 09:54:09 | 0       | 11003         |\n\n#### Customers dataset\n\n| Field Name | Customer ID | Name              | Gender  | Address              | Zip     | Sign Date  | Status  | Level   | Campaign | Linked with apps? |\n|------------|-------------|-------------------|---------|----------------------|---------|------------|---------|---------|----------|-------------------|\n| **Type**   | Integer     | String            | Integer | String               | Integer | String     | Integer | Integer | Integer  | Integer           |\n| **Example**| 10          | Joshua Threadgill | 0       | 10084 Easy Gate Bend | 66216   | 01/13/2013 | 0       | 1       | 1        | 1                 |"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\n\ntracks_schema = StructType([ \\\n    StructField(\"event_id\", IntegerType(), True), \\\n    StructField(\"customer_id\", IntegerType(), True), \\\n    StructField(\"track_id\", IntegerType(), True), \\\n    StructField(\"datetime\", DateType(), True), \\\n    StructField(\"mobile\", IntegerType(), True), \\\n    StructField(\"listening_zip\", IntegerType(), True)])\n\ntracks_df = (sqlContext\n              .read\n              .format('com.databricks.spark.csv')\n              .load('/tmp/tracks/tracks.csv', schema=tracks_schema))\n\ncustomers_schema = StructType([ \\\n    StructField(\"customer_id\", IntegerType(), True), \\\n    StructField(\"name\", StringType(), True), \\\n    StructField(\"gender\", IntegerType(), True), \\\n    StructField(\"address\", StringType(), True), \\\n    StructField(\"zip\", IntegerType(), True), \\\n    StructField(\"sign_date\", StringType(), True), \\\n    StructField(\"status\", IntegerType(), True), \\\n    StructField(\"level\", IntegerType(), True), \\\n    StructField(\"campaign\", IntegerType(), True), \\\n    StructField(\"linked_with_apps\", IntegerType(), True)])\n\ncustomers_df = (sqlContext\n                 .read\n                 .format('com.databricks.spark.csv')\n                 .options(header='true')\n                 .load('/tmp/tracks/customers.csv', schema=customers_schema))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["trackfile = sc.textFile('/tmp/tracks/tracks.csv')\n\ndef map_to_keyvalue(str):\n  r = str.split(\",\")\n  # key: customer_id\n  # value: array of songs listened by the customer\n  return [r[1], [[int(r[2]), r[3], int(r[4]), r[5]]]]\n\ntbycust = (trackfile\n            .map(map_to_keyvalue)\n            .reduceByKey(lambda x, y: x + y))"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["def compute_stats(tracks):\n  mcount = morn = aft = eve = night = 0\n  tracklist = []\n  \n  for t in tracks:\n    trackid, dtime, mobile, zip = t\n    \n    # list of tracks listened to\n    if trackid not in tracklist:\n      tracklist.append(trackid)\n    \n    # count tracks listened in different part of day\n    date, time = dtime.split(\" \")\n    hourofday = int(time.split(\":\")[0])\n    if (hourofday < 5):\n      night += 1\n    elif (hourofday < 12):\n      morn += 1\n    elif (hourofday < 17):\n      aft += 1\n    elif (hourofday < 22):\n      eve += 1\n    else:\n      night += 1\n\n    # count tracks listened on mobile\n    mcount += mobile\n    \n    return [len(tracklist), morn, aft, eve, night, mcount]\n  \ncustdata = tbycust.mapValues(compute_stats)\n\nfrom pyspark.mllib.stat import Statistics\naggdata = Statistics.colStats(custdata.map(lambda x : x[1]))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["cust_kv = custdata.toDF()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["unique, morn, aft, eve, night, mobile"],"metadata":{},"outputs":[],"execution_count":8}],"metadata":{"name":"train","notebookId":58746610960142},"nbformat":4,"nbformat_minor":0}
